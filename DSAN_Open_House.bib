@book{dignazio_data_2020,
  title = {Data {{Feminism}}},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  year = {2020},
  month = mar,
  publisher = {MIT Press},
  abstract = {A new way of thinking about data science and data ethics that is informed by the ideas of intersectional feminism.Today, data science is a form of power. It has been used to expose injustice, improve health outcomes, and topple governments. But it has also been used to discriminate, police, and surveil. This potential for good, on the one hand, and harm, on the other, makes it essential to ask: Data science by whom? Data science for whom? Data science with whose interests in mind? The narratives around big data and data science are overwhelmingly white, male, and techno-heroic. In Data Feminism, Catherine D'Ignazio and Lauren Klein present a new way of thinking about data science and data ethics---one that is informed by intersectional feminist thought.Illustrating data feminism in action, D'Ignazio and Klein show how challenges to the male/female binary can help challenge other hierarchical (and empirically wrong) classification systems. They explain how, for example, an understanding of emotion can expand our ideas about effective data visualization, and how the concept of invisible labor can expose the significant human efforts required by our automated systems. And they show why the data never, ever ``speak for themselves.''Data Feminism offers strategies for data scientists seeking to learn how feminism can help them work toward justice, and for feminists who want to focus their efforts on the growing field of data science. But Data Feminism is about much more than gender. It is about power, about who has it and who doesn't, and about how those differentials of power can be challenged and changed.},
  googlebooks = {zZnSDwAAQBAJ},
  isbn = {978-0-262-35853-8},
  langid = {english}
}

@article{ingold_amazon_2016,
  title = {Amazon {{Doesn}}'t {{Consider}} the {{Race}} of {{Its Customers}}. {{Should It}}?},
  author = {Ingold, David and Soper, Spencer},
  year = {2016},
  month = apr,
  journal = {Bloomberg},
  url = {http://www.bloomberg.com/graphics/2016-amazon-same-day/},
  urldate = {2024-01-31},
  abstract = {In six big cities, Amazon Prime Same-Day delivery doesn't serve black ZIP codes as well as it does white ones.},
  langid = {english},
  file = {/Users/jpj/Zotero/storage/6DMRE4S7/2016-amazon-same-day.html}
}

@techreport{schiebinger_machine_2020,
  title = {Machine {{Translation}}: {{Gendered Innovations}}},
  author = {Schiebinger, Londa and Klinga, Ineke and Paik, Hee Young and {S{\'a}nchez de Madariaga}, In{\'e}s and Schraudner, Martina and Stefanick, Marcia},
  year = {2020},
  url = {http://genderedinnovations.stanford.edu/case-studies/nlp.html#tabs-2},
  urldate = {2022-04-13},
  file = {/Users/jpj/Zotero/storage/PDZWL4Q5/nlp.html}
}

@inproceedings{wan_kelly_2023,
  title = {``{{Kelly}} Is a {{Warm Person}}, {{Joseph}} Is a {{Role Model}}'': {{Gender Biases}} in {{LLM-Generated Reference Letters}}},
  shorttitle = {``{{Kelly}} Is a {{Warm Person}}, {{Joseph}} Is a {{Role Model}}''},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Wan, Yixin and Pu, George and Sun, Jiao and Garimella, Aparna and Chang, Kai-Wei and Peng, Nanyun},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {3730--3748},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.243},
  url = {https://aclanthology.org/2023.findings-emnlp.243},
  urldate = {2024-11-22},
  abstract = {Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.},
  file = {/Users/jpj/Zotero/storage/55UV8HQD/Wan et al. - 2023 - “Kelly is a Warm Person, Joseph is a Role Model” .pdf}
}
